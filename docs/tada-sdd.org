* About this document 						     :draft1:
This document is intended for system software developers and
maintainers.  It may also be shared with decision makers to provide
a common context that includes requirements, goals, and plans.  The
primary audiance is employees of NOAO, but an attempt was made to keep
it general enough for use in other domains.

* Document Overview                                                  :draft1:
The software described here is: /TADA (Telescope Automatic Data Archiver)/
The purpose of TADA is to provide an end-to-end flow of data from
telescope instruments to archives at machines around the world. By its
very nature, TADA is a multi-machine system.  The *general purpose*
might be described as:
   #+BEGIN_QUOTEb
   Collect data from remote instruments and save in nearby
   cache. Transfer cache data across network with dubiuos reliability
   to a local archive. Remove cache data after it has been
   successfully transferred.
   #+END_QUOTE

This System Design Document (SDD) attempts to describe software as
built. Since the document and software are being written at the same
time, they may not match until the project is completed. (Maybe not
even then!  If in doubt ask me (Steve P).

The project must proceed without the benefit of known
requirements. Therefore, development will proceed with a series of
alpha releases that represent various "threads through the system".
Each thread may contain arbitrary amounts of mock-component code but
will at the very least simulate the end-to-end flow of data.  Later
alpha-versions will contain progressively more real (non-mockup)
components. The first beta release will contain no mockup-components
for the thread but may contain mockup-components on either side of the
thread. e.g. A mockup-component will pretend to be telescope
instruments sending data.

At each alpha-release, stake-holders have the opportunity to affect
what is done for the next release.  Its *very important* for
stake-holders to provide prompt feedback when needed.  In the absence
of such feedback, developers will simply guess on what is important
next and move on.

We refer to "mountain" and "valley" sites/machines.  These aren't
necessarily to be taken literally. "Mountain" means the computer(s)
connected to telescope instruments directly or through LAN. The
connection is assumed to be unlikely to be lost unless the
instruments and computers go down also.  "Valley" means the
computer(s) far away from the telescope instruments and connected
only via the internet. 

The software described here is intended to solve a general problem --
collecting remote data and storing in an archive without loss in the
face of failures in hardware and software.  But the document focuses
on that problem in a more specific domain (optical astronomy data).
Where there are domain specific issues, they are described elsewhere.
Links in this document may refer to those domain specific issues. For
non-NOAO readers, those links will be unresolvable.  If you are
reading this for an application that does not involve mountain-top
telescopes, you should be able to translate.  For a more general
application or domain, you can roughly think:
  - telescope :: "data collection instrument"
  - mountain-top :: "collection point for several instruments"
  - Mountain Cache :: "storage for several instruments" that is very
                      reliably connected to the instruments
  - Mountain Mirror :: "off-site storage for several instruments". Not
       reliably connected to instruments. 

* TODO Next up (rough prio order) [9/15]
- [ ] different dir (irods) structure for ingest? 
  + use subdir names from SB_* fields
- [ ] special user or group for this to all run under?
- [ ] avoid having to do "iinit"
- [X] rename puppet-dci2 (puppet-tada, puppet-astro, puppet-noao)


- [X] install all python in github (and PyPI) in pythonic way
  + use wheel
  + universal format
  + see "distributing" check-list
  + AS DONE: not in PyPI

- [ ] Real ingest into archive ("PAT" area)
- [ ] multi: telescope, mountain, valley machine tests
  + one config to rule them all (for multi T,M,V case)
- [ ] connect two valley machines (share archive)

- [X] make sure all logs go to /var/log/tada/
- [X] make sure all configs come from /etc/tada 
- [X] make sure all services write pid to /var/run/tada

- [X] replace stubs (tada/submit.py)
  + AS DONE: but archive_ingest talks to FAKE server (mock_ingest.py)
- [X] Write management script clear/reset everything:
  : source /sandbox/demo/clean-valley.sh
- [X] CLI program (or cmdline) to resubmit all from inactive
  : dqcli --redo
- [X] document underlying data structure 
  + This includes all the /var/tada/* subdirs, the irods
    "/tempZone/valley" structure, and 2 queues
- [ ] Keep last N on Transfer Success (Maintenance Point)
* TODO next phase  [0/16]
- [ ] change irods: 
  + demoResc -> tadaResc
  + /tempZone -> tadaZone
- [ ] Combine services (dqpush, dqpop) into a single "dataq" service
  + which can started/stopped/restarted in the normal linux way.
- [ ] write hook for use by Pipeline; see [[https://bitbucket.org/noao/opswiki/wiki/ICDs/Pipeline-submit%20][ICD]]
- [ ] verify "maximum_errors_per_record" works for value > 0
- [ ] mechanism to bundle instrument specific data with file for
  downstream use in modifying header 
- [ ] formalizing logging 
  + logger config
  + option to save to /etc AND to stdout
  + clean out old debugging output
- [ ] clean out debugging
- [ ] new data-flow diagram 
  + abstracted to emphasize:
    - TWO queue/machine types
    - N processes/threads
  + flow fork based on action pass/fail, validation pass/fail
- [ ] try print from old machine (verify LPD protocol installed)
  + (did this before but not in this thread
- [ ] scrape all code for "!!!" indicator of stuff that needs fixing.
  + cd $SANDBOX
  + find data-queue tada -name "*.py" -exec grep '!!!' {} \; -print
- [ ] exception handling everywhere
  + There's some but nowhere near enough. There should be no way a SVC
    should be able to die with exception.  Should be nothing that could
    be sent to it via its port that would kill it. Not there.
- [ ] Prepare for formalize testing by Pat
  + change thresholds in config, run to exceed them
- [ ] Allow cfg["maximum_errors_per_record"] to be infinity
  + for mountain
- [ ] Notify on error
  + via "dashboard" (harder, any can see state); OR
  + email (easier, adr via config)
- [ ] rule based validation before submit to Archive Ingest
- [ ] incorporate current telescope schedule (from web service)
  + cache first query for day. Means very late additions to schedule
    will be missed.
  + use funky Perl interface first version, grab XML result, use in Python

* TODO Deferred
- transfer Valley <==> Valley
- keep log of basenames; don't submit if grep finds
  + see "Keep last N on Transfer Success"
- refactor puppet
- Write monitor scripts to scrape from logs and queues
  + ERRORS and WARNINGS
- add puppet modules/classes for my stuff (dq,tada,astro)
  + After taking puppet class
- port rule based FITS validation from Archive Python code to Python
- allow astropost to handle list of files at once
  + multi-file rsync
  + multi-entry push to Valley Submit queue
* TODO Installation
* TODO Work-flows
** FITS Mitigation (NOAO)
When the Submit queue on a Valley machine runs the submit (ingest)
action it separates input files (records) by file type.  Only FITS
files will be ingested into the archive. Others will be moved to the
local NOARCHIVE[fn:5] directory.

Ingest of FITS files may fail due to serveral possible reasons:
1. Header contains in sufficient fields/values for ingest
2. The PROPID given in the header is not in the local DB mainained by
   the archive

No mater the reason for FITS ingest failure, failure N (!!!) times
will cause it to be put on the INACTIVE list of the Submit queue. An
operator can view the inactive list by executing the following command
on the Valley machine.
: dqcli --list inactive

An operator should monitor the Inactive list.  Anything on the list
*failed* to ingest.  It will /usually/ require manual intervention to
fix the file and ingest.  Under some circumstances, a retry of a file
will work later without modification.  Once such circumstance may
occur if the PROPID referenced in the FITS file was not initially in
the archive DB, but has been added since the failure.  It is 
/always ok/ to resubmit the entire inactive list in hopes that some of
the files will ingest.  To do so, execute the following on the Valley
machine:
: dqcli --redo
: dqcli --list inactive

Whatever is still on the inactive list should be modified so that it
can be ingested. Once modified (in place), resubmit using:
: dqcli --redo

*** WARNING: possible corruption
Modifying in place will change the checksum which may be used by irods
and is definitely used data-queue.  The work-flow needs some
enhancements to allow for:
1. files (checksum) changes
2. ability to delete a file instead of fixing and resubmitting

** Pipeline submit
* TODO Design Overview
The TADA system consists of a set of processes that communciate with
each other across multiple machines. 

** TERMS
Terms relevent to this section:
- Mountain Cache :: temporary storage for all data files accepted from
                    any telescope on the same mountain as the cache.
- Mountain Mirror :: duplicate of the Mountain Cache, but moved off of
     the mountain (to someplace logicaly near the archive). Cache and
     Mirror are seperated by a possibly unreliable network connection.
- Archive Staging :: data files vetted, scrubbed, and ready for archive ingest
- Mitigate :: data files that need to be corrected before they can go to
              Archive Staging
- Non-Archive :: files not suitable for archival. They will be deleted
   from here using a First In First Out (trashed) method.
- Transfer Queue :: Data-queue whos contents represent data files that
                    need to be moved from Mountain Cache (on
                    Mountain) to Mountain Mirror (on Valley).
- Submit Queue :: Data-queue whos contents represent data files that
                  have been submitted for saving. File types
                  that are not appropriate for the archive, will be
                  moved to a non-archive store. Else they'll be
                  put in archive (if possible) or in "Mitigation
                  Queue" (if invalid for archive submit)
- Mitigation Queue :: Data-queue whos contents represent data files
     that should find their way to the archive, but have something
     wrong with them.  After they have been manually modified, they
     should be put back on the Submit Queue

** ASTROPOST Process: Store file submitted via "lp" in Mountain Cache
*** Summary
Captures files sent from telescope via "print".
*** Description
From the telescope, a user or program uses the command:
: lp -d astro <filename>
to submit data.  This process is a [[http://www.cups.org/documentation.php/man-backend.html][CUPS backend]] ("astropost") and honors
the CUPS API.  It simply copies the file to a location under the mountain
cache root directory that is determined by backend parameters (user,
jobid, etc.) and adds the full name of the moved file (with checksum)
to the Transfer Queue.

*** Preconditions
The "lpd" protocol handling of CUPS must be enabled (it isn't enabled
by default).

*** Postconditions
The mountain cache directory tree is populated with printed files.
Uniqueness *of path* is maintained by the combination of jobid and
username. Its still possible (likely) for there to be duplicates of
base filenames in the tree.

** Process: Transfer content of Mountain Cache to Mountain Mirror
*** Summary
Transfer all data from Mountain (Mountain Cache) to Valley (Mountain Mirror).

*** Description
When simple, avoid transfering large files that have already been
successfully transfered. (e.g. rsync)

** Process: Morph Mountain Mirror to Archive Stage
*** Summary
Renames and copies files from the Mountain Mirror directory tree to
the Archive Stage directory tree.

*** Description
Uses fields from FITS headers to create new fields and create a filename
that satisfies the [[http://ast.noao.edu/data/docs][file naming convention]].  Maintains "backward
pointing" fields in the FITS header so that the path to the same data
on the Mountain Cache can be reproduced.

*** Preconditions
Mountain Mirror directory tree is on local machine file system.

*** Postconditions
All data files from the Mountain Mirror exist in one of three
places. See TERMS above for description of what these contain.
1. Archive Staging
2. Mitigate 
3. Non-Archive 

** Process: Remove confirmed transfers from Mountain Cache
*** Summary
Remove files from the Mountain Cache if they can be confirmed to exist
on the Valley machine (in Mountian Mirror). 

*** Description
Use checksum comparison to determine if file was transfered ok.
There may be considerable delay between when a file appears in the
Mountain Mirror and it is deleted from the Mountain Cache.  (But don't
count on it!) Mountian machines must have sufficient storage to
weather a long disconnect between Mountain and Valley machines. 

* TODO Config
** /etc/tada/dq.conf
The /dq.conf/ file is used to configure the values listed below. See
/dq_config.json/ for an example.

| Field                    | Purpose                                        |
|--------------------------+------------------------------------------------|
| dirs.log_dir             | Location for all log files produced by TADA    |
| dirs.run_dir             | Contains PIDs for running apps/services        |
| queues.name              | Named queue                                    |
| queues.type[fn:6]        | Indicates queue purpose/location               |
| queues.action_name       | Action to do on files popped from queue        |
| maxium_errors_per_record | Automatically retry action this many times     |
| maxium_queue_size        | More than this # of items on queueraises error |
|--------------------------+------------------------------------------------|
| next_queue               | Push successful pops to this queue             |
| cache_dir                | Location of cached files on mountain machine   |
| mirror_irods             | iRODS path. Mirror of cache on valley machine  |
|--------------------------+------------------------------------------------|
| archive_irods            | iRODS path to files to be stored in archive    |
| archive_dir              | location archive files on valley machine       |
| noarchive_dir            | location nonarchive files on valley machine    |




When the configuration file is first read, basic validation is done to
make sure the expected fields exist. Litttle or no validation is done
against field *values*, however.

The same configuration file should be installed on all
machines. Machine specific variations are determined by the
"queue.name" which is specified when the data-queue services are
started.  (*NOTE:* Rather than use the current command line option
method for specifying queue-name, a local machine-specific config
should be added and used!!!)

*** WARNINGS
- (some) Directory names in config and provisioning must match
- (some) IRODS paths in config and provisioning must match

** TODO iinit
irodsHost valley
irodsPort 1247
irodsUserName rods
irodsZone tempZone
** TODO irod directory structure

#+BEGIN_EXAMPLE
/sd_zone/
        from_cache/
        for_archive/
#+END_EXAMPLE

* As Built
** General
This section documents specific builds.  When a requirement or feature
is described outside of the /As-Built/ section, it should be
considered a future possibility, *not* something that has been
implemented. 

I record dated sub-sections below but will typically hide all but the
most recent.  Ask if you want older sections for some reason.

** Touch Points
- INPUT queue
  via lpd protocol
- INPUT to submit queue (after mitigation, from pipeline)
- INPUT telescope schedule from web service
- OUTPUT to archive ingest
** Changes from iDCI
- All files "printed" to printer "astro" are sent to valley (not just
  selected types)
- No gratuitous waiting or "spinning"!
  Data flow from "print" to submit to archive never involves arbitrary
  wait. The flow is data driven, so that as soon as one process
  finishes with it, the next process does its job (provided it isn't
  already working on another file).  No CRON jobs are used for any of
  the main data flow.  Some CRON may be introduced for optional pieces
  (such as monitoring). 
  
** TADA details <2014-12-23 Tue>
** What is put in iRODS
- datatype added (isysmeta) for:
  + "FITS image"
  + "jpeg image"

** Thread-4 <2014-11-23 Sun>
*** Data Stores
1. Mountain:/var/tada/mountain_cache/
2. irods (Valley) /tempZone/valley/mountain_mirror
3. Valley:/var/tada/archive
4. Valley:/var/tada/mitigate
5. Valley:/var/tada/no-archive

*** Data Queues
- Mountain:transfer
  + transfer file from Mountain to Valley using irods when irods is
    available ("always", except for network trouble)
- Valley:submit
  + submit FITS files to archive.  Keep on queue (but inactive) if
    error
  + move non FITS files to no-archive directory without change
  + inactive = Mitigate

*** Features (mark the ones that are acceptable)
1. [ ] Print of duplicate files, captures all (unless real quick).
   If a file is repeatedly printed, its duplicate will go through the
   system. Each file has unique storage (mostly)
   since its PATH contains User and Job-id of the print.  With
   multi-domes and the same user on each dome, files could collide.
   For instance: if all domes use an indentical username for lp, AND
   the print queues across domes are counting jobs in the same range
   (colliding job-ids), AND users print files with the same name from
   different machines, THEN we get collisions will will result in
   overwrite. If a single user from one machine does two prints in a
   row on the same file, the first may still be in the DataQueue when
   the second is printed.  In this case the 2nd will be ignored.  In
   this case its only the checksum (i.e. the content) that has to be
   the same for the second to be ignored.

2. [ ] There is no way to resubmit from mountain for replacement in the
   archive.

3. [ ] Directories remain when files moved/removed
   When files are moved/removed (e.g. mountain_cache cleared after file
   recieved in valley), their directories remain.  The directory is of
   form: /cache/<user>/<job-id>/  It could be argued that keeping the
   directory provides an audit trail of sorts.  Downside is nothing is
   cleaning up those directories.  Since they don't have files, they
   take up very little space.  Perhaps a cleanup cronjob should remove
   old and empty dirs [DEFERRED]  This effect leaves "audit" traces in:
   + Mountain:/var/tada/mountain_cache/
   + Valley:/var/tada/archive/
   + irods /tempZone/valley/mountain_mirror/
     - Note: the base filename in mountain_mirror is different than the
       corresponding filename in archive because the act of submitting
       causes a rename to match file naming standards.

4. [ ] Original file names retained until (before) submit to archive

5. [ ] Renamed FITS files also have their headers modified (augmented)
   We end up with identical astronomical content (raw data) in two
   files. The two files have similar paths. "Similar" means different
   root, identical "<username>/<job-id>" directory tail, and different
   basename. The raw version has fewer header fields and the original
   file basename.  The modified version has added header fields and is
   renamed to filename standards.

6. [ ] Configuration
   Uses a single /etc/dataq/dq.conf file for configuration of:
   + log, run (pid) directories
   + named queues
     - port, host
     - action name associated with queue (definition of actions are in code)
     - max errors allowed for automatic resubmit to queue (not tested)
     - max queue size 

7. [ ] File deleted from mountain_cache as soon as transfered to Valley
   Immediately upon successful transfer of file from
   Mountain:/mountain_cache to Valley:/mountain_mirror (per irods), it
   is deleted from cache.

8. [ ] Failed actions move to "inactive"
   When fits file fails submit, it is moved to Mitigate store.
   It should also be moved from Active to Inactive on the Submit
   queue. Code allows batch reactivate.  

9. [ ] ?? TRANSFER fail goes to inactive?
   Have to simulate network connection break.  Haven't tested. Might
   work. But it works for SUBMIT queue when submit action fails.

*** Known Problems <2014-11-21 Fri>
- No consistent logging
  The logging from the pieces are not brought together in single
  unified way.

- Does not actually submit to archive (simulates only)
  This will be tricky.  To be added in next release (the first MVP). 

- Not clearing /tempZone/valley/mountain_mirror/ after:
  + move of file to /var/tada/no-archive
  + success of submit (should be DELETED???)

- irod client setup ("iinit")
  Provisioning does not automatically setup the "vagrant" user
  as an irods client.  I think this has been done for a lesser user so
  probably just need to move provisioning.  For MVP this will have to
  formalized into specific TADA unix user and associated access
  rights, provisioning, etc.

- Services (dqpush, dqpop) can crash
  They are not protected against crash.
  There should be no way for them to die on error (raise
  exception). It should be impossible to send data on port that would
  cause service to die.  It should be impossible to push/pop items
  from queue that would cause service to die.

- Software not installed
  Provisioning does not install software being actively developed.
  These will be uploaded to github and PyPI so will install just like
  other open source python packages currently are.  For now, I do it
  in a local way. (for quicker development).  The packages are:
  + dataq
  + tada

- There are no tests
  The only thing that is remotely like an automatic test is
  "sandbox/demo/demo.sh".  It: cleans the slate, initializes, runs a
  few files through, shows results.  It does NO checking of results.

- (maybe not problem) All records on queue should be reflected in
  exactly one of Active, Inactive.  Have not confirmed this.

*** COMMENT ???
- Attempt to post a duplicate file will be ignored
  + "duplicate" is determined by checksum of content. Filename is irrelevent.
- The same filename with different content can be printed to
  "astro". Since the full pathname makes use of user and job id, no
  collision will occur in Mountain Cache or Mountain Mirror.
- Upon successful transfer of a file from Cache to Mirror, the file
  will be immediately removed from the Cache. (if longer lived copies
  are wanted on the mountain, they can be done with a seperate process).
- On failure to transfer a file from Cache to Mirror, the file will be
  retained in the Cache and retained on the transfer queue with an
  incremented error count.
*** Requirements met
- [X] all software committed to github repositories
- [X] reproducible installs
  + single line script against source repo.
- [X] insert "archival metadata" just before final archiving
  + Define this more precisely
- [X] filename agnostic; nothing in the system depends on the
  structure or uniqueness of a filename 
  + up to call to archive ingest; archive ingest may violate
- [X] Rename fits files per standard using header values
- [X] Continue to store on mountain if connection to valley is severed.
  + How long? [DEFAULT ANSWER: 7 days] Currently; indefinite
  + NOT: Automatically dump stored mountain data to valley when connection
    restored
- [X] Insufficient metadata in FITS causes files to be moved to
  Mitigation. 
  + Required raw fields:
    - DATE-OBS
    - INSTRUME
    - OBSERVAT
    - OBSID
    - PROPID
    - PROPOSER
  + Required cooked fields (just prior to ingest):
    - DATE-OBS
    - DTACQNAM
    - DTINSTRU
    - DTNSANAM
    - DTPI
    - DTSITE
    - DTTELESC
    - DTTITLE
    - DTUTC
    - PROPID
- [X] Eliminate use of STB
- [X] Eliminate use of cron-jobs for main data-flow
- [X] Provide high-bandwidth transfer Mountain -> Valley
  + Uses parallel iput
- [X] No machine specific code; variations held in config file
  + There is different installation per CLASS of machine (Mountain, Valley)
- [X] Update metadata to contain following fields:
  + DTACQNAM
  + DTINSTRU
  + DTPI
  + DTSITE
  + DTTELESC
  + DTTITLE
  + DTUTC
  + SB_DIR1
  + SB_DIR2
  + SB_DIR3
- [X] Files failing submit to archive move to Inactive of Submit queue
  
** COMMENT <2014-10-24 Fri>
*** Thread-2: Touches FITS data  (verifies selected metadata in archive)
Given a "source directory" tree that may contain FITS files, 
*** Open Issues
- Which files from input list ("printed" files) should get moved to archive?
  + DEFAULT ANSWER: only *.fitz.fz

- What if a FITS file does NOT contain minimum required metadata?
  + DEFAULT ANSWER: Reject file, move to remediation store, log error

- What is the minium required metadata?
  + DEFAULT ANSWER: Presence of following fields in FITS hdr without
    regard to their value:
    - DATE-OBS
    - DTACQNAM
    - DTINSTRU
    - DTNSANAM
    - DTPI
    - DTSITE
    - DTSITE
    - DTTELESC
    - DTTITLE
    - DTUTC
    - PROPID

** Caveats and Warnings
- Assume irods documentation is correct when it says that transfers
  are guaranteed using checksum.  I have not done an experiment to
  prove this.
- It is possible for a queue push to fail (perhaps the queue service
  was killed). If so, there may be items in the associate storage that
  are not in the queue.  See "Deferred" below for how to handle this case.
** Deferred
- Process to monitor error counts on queues.  Demand human attention
  for any files that get high (config setting) error count.
- Process to compare queue and associated data storage.  Add items to
  queue that aren't there already but are in storage.
- dq: dbvar.py => constants.py; change names to UPCASE (in
  constants.py and <user>.py
- Add redis host:port to dq.config

* OPS visible file flow.  aka: "Where did the file go?"
Every file posted ("printed") to astro goes somewhere.
#+BEGIN_SRC dot :file figures/tada-fileflow.png :cmdline -Tpng 
  digraph fileflow {
      astro [shape="invhouse"];

      cache [label="mountain:/var/tada/mountain-cache/"];
      noarchive [label="valley:/var/noarchive/"];

      node [shape="box"];
      mirror [label="valley:/tadaZone/mountain-mirror/"];
      archive [label="valley:/tadaZone/archive/"];
      
      astro -> cache [label="lpr -P astro <filename>"];
      
      cache -> cache [label="no Valley"];
      cache -> mirror [label="Valley irods accessible"]

      mirror -> noarchive [label="non-FITS file"];
      mirror -> archive [label="Successful Submit to Archive"];
      mirror -> mirror [label="Inactive after N unsuccessful Submits"];

  }
#+END_SRC

* Diagnosing problems
Its guaranteed that there will be no problems!

Ok, maybe there will be.  If so, this sections lists ways that might
help you can find the source.

** Turn on debugging output
Most command line invocations support the "/loglevel/" argument.  Set
it to /DEBUG/ to get maximum output.  Example:
  : dqsvcpop --loglevel DEBUG --queue submit
Some places in the code catch Exceptions and emit a stack traceback
only if the loglevel=DEBUG.  

** TODO Simulate (NEEDS UPDATE)

There is a simulator of the data flow in: [[https://github.com/pothiers/daflsim][daflsim]]. This can be used to
experiment with more radical changes to the parameters and topology of
the data-flow with zero risk of breaking anything.  Of course, since
its a simulation, it will only give approximate results.

*NB:* This code was was written to aid in understanding the previous
legacy data-flow (iDCI).  As of <2014-12-18 Thu> it has not been
updated to reflect the new data-flow.

* COMMENT Sprint user stories
These are the expect outcomes from progressively more complex [[https://www.scrum.org/][scrum]] sprints.

In our case "user" means two kinds of people: 
  1. scientist that want access to data,
  2. SDM DevOps employees that need to manage the process

** Thread-1: Establishes file move to archive and test
This is minimal "thread through the system" starting at raw-data and
terminating with files in the archive.
- [X] mock-LPR;  Feed each file in list to Ingest after random delay
- [X] Ingest;  Copy file into mock-IRODS (a local filesystem)
- [X] Test;  Verify all input files are  in mock-IRODS

*** 
#+BEGIN_SRC dot :file figures/thread1.png :cmdline -Tpng :tangle src-tangles/thread1.dot
  digraph thread1 {
      rankdir="LR";
      edge [len=1.0];
      raw [shape="invhouse"];
      expected [label="Expected\n(raw)", shape="invhouse"];
      report [shape="house"];

      raw -> mockLpr -> ingest -> archive -> test;
      timing -> mockLpr;
      expected -> test -> report;
  }
#+END_SRC

** Thread-2: Touches FITS data  (verifies selected metadata in archive)
- [X] all of Thread-1
- [X] only transfer files matchin: *.fits.fz 
- [X] insure minimum (level 0) set of required metadata fields in FITS
  + minimum acceptable for archive
- On inadequate metadata:
  - [X] reject (don't archive) 
  - [ ] move to remediation store
  - [ ] log error
- [X] Test;  Verify all files in mock-IRODS contain required metadata;

*** 
#+BEGIN_SRC dot :file figures/thread2.png :cmdline -Tpng :tangle src-tangles/thread2.dot
  digraph thread2 {
      rankdir="LR";
      edge [len=1.0];
      raw [shape="invhouse"];
      expected [label="Expected\n(cooked)", shape="invhouse", fontcolor="green"];
      report [shape="house"];

      raw -> mockLpr -> ingest;
      ingest -> archive [label="insert metadata", fontcolor="green"];
      archive -> test;
      timing -> mockLpr;
      expected -> test -> report;
   }
#+END_SRC
    
** Thread-3: Split into 2 machines, use iRODS client/server
- [ ] mock-LPR;  Feed each file in list to Ingest after random delay
- [ ] Ingest; add file to iRODS[fn:3] on remote machine
- [ ] Verify integrity of file across machines (checksum)
  + Retry N times if integrity violated
- [ ] Test; Verify all iRODS filesystem contains everything from orig
  filesystem



*** 
#+BEGIN_SRC dot :file figures/thread3.png :cmdline -Tpng :tangle src-tangles/thread3.dot
  digraph thread3 {
      rankdir="LR";
      edge [len=1.0];
      raw [shape="invhouse"];
      expected [label="Expected\n(cooked)", shape="invhouse", fontcolor="green"];
      report [shape="house"];
      archive [label="Archive\n(iRODS)", shape="box"];

      subgraph cluster_mountain {
        label = "Mountain";
        style="dashed";

        timing -> mockLpr;
        raw -> mockLpr -> ingest;
      }

      subgraph cluster_valley {
        label = "Valley";
        style="dashed";

        ingest -> archive [label="iCommands", fontcolor="green"];
        archive -> test;
        expected -> test -> report;
      }
   }
#+END_SRC

** LATER
- easy to add plugins for scientists 
  + scientist provides program to run against (filtered) set of
    images, stores "result" file accessable in archive
* Classes of supporting machines (hosts)
The machines that are used in the TADA system can be categorized into
the following classes of hosts. The software that runs on each machine
of the same class should be identical and come from a single source
code repository.  Any difference between the behavior of
software on different machines of the same class comes from
configuration files unqiue to the machine.

 - T :: Telescope; The machine from which observer does the "print". We
        *can't touch this* except to add a printcap entry.
 - M :: Mountain cache; Contains all instrument data that hasn't
        successfull made it into the archive. And maybe some that has.
 - V :: Valley; The first stop of data coming from Mountain
 - A :: Archive; the final resting place of the data made available to
        scientists. We *can't touch this* directly. Only by "submit to
        ingest". 

Roughly, data flows top to bottom through the classes of machines
listed above.  Meaning; data is generated at the Telescope, gets
collected at Mountain cache, then transfered to the Valley, and
finally scrubbed and submitted to the Archive.

There are more than one instance of each of these classes of hosts, so
things get a little more complicated with regard to collecting and
distributing. 

Here's a rough schematics of what we end up with.  Arcs represent data
flow.  Note that data only flows bewteen "adjacent" classes of hosts.[fn:4]

#+BEGIN_SRC dot :file figures/general-machine-schematic.png :cmdline -Tpng :tangle src-tangles/thread1.dot
  digraph schematic1 {
      rankdir="LR";

      T1 -> M1 -> V1 -> A1 ;
      T2 -> M2;
      T3 -> M3;
      {M2;M3} -> V2 -> A2 ;
      A1 -> A2 -> A1;
  }
#+END_SRC

* Goals                                                              :draft1:
** Prove its done right
To PROVE we have it right[fn:1], we need good monitoring. To support
courageous code changes, the monitoring should be nearly identical
between:
- production
- developmental (to be deployed) system (on VMs or real machines)
- under DES (Discrete Event Simulation)[fn:2]
  [[~/sandbox/dfsim/dfsim.py][dfsim]]
** Easy to maintain
Create a system that can be maintained using no more than 25% of one
full time employee.  We expect maintenance to include:
- correcting problems in FITS files stored in Mitigation queue and store
- replacing broken hardware (disks, computers) and installing required
  software from scratch

*** Simulation                                                     :noexport:
It would be GREAT to generally connect simulator to data-flow graph
display. What tools?  Need graphics that support drawing graph and can
hilite nodes. tcl/tk?  Is there something in latest networkx that
helps? Perhaps I need to write a general OSS project.  Lauch with
graph. It draws.  Pipe in for commands (hilite, others?). Pipe out for
state?

*** Monitor display                                                :noexport:
Plots from DES (gnu plot?) to represent values of resources (queue
size).  Alerts for when thresholds exceeded. (queue max size reached)
Utilization measures.
* Secondary Goals                                                    :draft1:
My primary goal is to develop useful software.  Exactly what that
software will be is unfolding.  It has to be an iterative process. But
regardless of what the software is, there are some secondary goals
that go along with it. Here are most of them:

1. Documentated as built

   My intent is to provide "as built" design and code documentation. Code
   documentation will be generated directly from annotated code. Design
   docs will be hand written, with diagrams.  It will include example
   runs with inputs and outputs listed. The intended reader for both is
   someone that is software tech savvy.

2. Requirements addressed in software as built

   Whatever I develop is intended to address some requirements that I
   have in mind.  I'll put those down in a document.  These may be
   different than any requirements anyone gives to me because they will
   be directly focused on functionality of the software I develop, rather
   than on a larger system perspective (which I may have little control
   over). The intended reader is management and/or software engineer.

3. Tests

   Each package I write has a "smoke test".  This is a simple script that
   can be run by anyone after the software is installed to see that it
   works in some fashion.  My smoke tests are not exhaustive regression
   tests.  They are intended to be used by developers to ask the
   question: "did I break anything with the last change". Smoke tests
   include their own test data and are checked into configuration
   management with the code.

4. Configuration Management

   All my software will be checked into github or bitbucket. Related
   documentation will be included with the code.

5. Auto provisioning of everything I develop

   I'll provide a "vagrant box", or similar, for all my stuff.  This will
   allow a new Virtual Machine(s) to be created from scratch and all my
   stuff installed on it such that my smoke tests will work on the new
   VM(s).

6. Documentation of existing system

   In the process of figuring out what my new stuff has to do, I have to
   figure out what the existing stuff does. I don't want to attempt to
   hold all that in my head, so I document it.  You've all seen at least
   part of my DCI "notes". That is basically the source of what I'm
   talking about here.  I don't intend to formalize it any way unless
   forced into it. I think it would be too time-consuming/expensive for
   me to do and I think I have more the enough technical work on my
   plate.  But I will provide at least a crude extraction from my notes
   to something that might be useful to others.  The effort I put into
   such depends on feedback from you. No feedback means I'll provide
   something that is a similar level of informality as the notes I've
   already shared with you. I've already exported some parts of that
   (like my diagram) to the opswiki.

7. Keep It Super Simple
   
   Work very hard to keep the structure of the system and code
   simple.  If there is a temptation to "optimize", make sure its
   worth it. To be worth it, there must be an existing case of
   inadequate performance and a requirement must exist to perform at a
   quantifiable level that the better than the current one. Before
   changing code, *measure* the system to identify where the ill
   performing area is.  Don't add optimatizations unless they are
   *proven* to help meet requirements.

---------

* TODO Requirements
** General systemic requirements
1. Provide all required functionality of tje system this replaces
2. Resilient 
   - don't break -- EVER
3. Maintainable
   - by new employees without large learning curve, under 50% FTE
4. Operate fast enough (need quantification)

** Candidate requirements
These requirements have *not been committed to*.  In many case they
need to be made more precise.

- [ ] All database clients must be capable of reconnecting to database
  servers on connection loss (so components can be restarted)
- [ ] Increase level of automation of regular operation functions
- [X] use version control always; with commit comments
  + Stored in github at: ???
- [ ] elliminate direct changes to live production system (from tagged version)
- [ ] (document minimumaly acceptable coding style)
- [ ] Implement regression testing (automated where possible, documented otherwise)
- [ ] write design documentation
- [ ] write installation documentation
- [ ] write usage documentation
- [X] reproducible installs
- [ ] daily operations must not require manual intervention
- [ ] daily operations must not require human monitoring (automatic alerts instead)
- [ ] eliminate metadata remmediation in its present form (what form???)
  + get metadata from file format, or
  + get metadata from TO/observer/observatory support staff at data
    collection time
- [X] insert "archival metadata" just before final archiving
- [ ] insert of archival metadata should be idempotent
- [ ] eliminate mountain copy coherency requirement (???)
- [X] filename agnostic; nothing in the system should depend on the
  structure or uniqueness of a filename
- [ ] limit access to internals connection points (ports, databases)
  + perhaps by host, port, user
- [ ] literate programming: data flow software and config files: must be
  able to auto generate a document that describes the flow (including
  connectivity or data-flow diagram).
- [ ] Continue to store on mountain if connection to valley is severed.
  + [ ] How long? [DEFAULT ANSWER: 7 days]
  + [ ] Automatically dump stored mountain data to valley when connection
    restored
- [ ] Mountain machines run unattended. Disk "never" overflows.
  + Data that has been successfully transfered to valley is deleted
    from mountain.
  + If connection to valley remains severed for extend time and data
    continues to be collected on mountain, data will be lost.  How?
    [DEFAULT ANSWER: oldest will be thrown away first]
- [ ] Data submitted to NSA (archive) must have PROPID that is in the NSA
  metadata-DB
  + How is NSA metadata-DB retrieved
  + What if PROPID is not in metadata-DB? [DEFAULT ANSWER: File is
    moved to remediation store; error logged; no ingest happens]
- [ ] Handle "typical" failure modes gracefully with no loss of data:
  + reboot of any machine at any time [IMPORTANT - automate test?]
  + Lost of DNS
  + filesystem corruption (within "reason")
- [ ] Verify no errors on submit of file to archive (NSA) via socket
  + How?
  + What does NSA return back?  Does it return error for every case in
    which file is not archived?
- [ ] Same version of iRODS in TADA as NSA?
  + Not required if API is identical for used commands. 
  + [[http://irods.org/][iRODS]] says that version 3.+ and 4.+ can be combined in one collection
- [ ] Security ???
  + firewalls configured to only allow access to key ports from
    trusted hosts
- [X] Files must be renamed according to TBD scheme before submit to
  archive
  + How is name derived? 
  + Assume name is derived from header -- but this limits to
    processing of FITS (known header info) only.
- [ ] allow disabling of auto cache-file expiration
- [ ] on "submit to archive" retry N times (N given by config file)
- [ ] tests to include simulation of irods stop-delay-start

** From 2010 iDCI project definition
(minor editing done on language of requirements)

#+BEGIN_EXAMPLE
iDCI: Integrated Data Cache Initiative
Version 0.1 (02/24/2010)
The [[http://chive.tuc.noao.edu:8080/DPPDOCS/operations-documentation/software-system/application-components/noao-e2e/e2ev1.5/iDCI_project_definition.pdf/at_download/file][PDF]] contains a bit more detail on each requirement.
#+END_EXAMPLE

*Status* below is per Irene.  Some might not be true anymore. 

1. Retain the existing DCI configuration, physical and logical
   resources. 
   *Status: Satisfied*
   + [-sp-] Need to retain physical resources, but why the logical ones?
2. Implement design changes that lower Operations maintenance while
   maintaining the overall functionality of the existing DCI.
   *Status: Not Satisfied*
3. Provide an interface for external E2E boundary objects.
   *Status: Satisfied*
   + [-sp-] I don't see a well defined/documented interface.
4. Guarantee the reliable and immutable transfer of data between all start and
   end points controlled by the iDCI.
   *Status: Satisfied*
   + [-sp-] Not happening, unless requirement allows for manual fudging
5. Maximize use of available bandwidth for bulk data transfer without
   interfering significantly with normal network traffic.
   *Status: Satisfied*
   + [-sp-] Why?  Certainly not "maximized" (maybe "improved")
6. Persist the state of pending data transfers across network outages, system
   failures and unexpected crashes of the software, recovering automatically once local or
   remote services become available.
   *Status: Satisfied*
   + [-sp-] Not happening.  People regularly have to start/restart pieces.
7. Be configurable to as to provide flexible routing of data to alternate sites.
   *Status: Satisfied*
   + [-sp-] At what touch point?  I don't see any way of doing this
     simply by changing a config file.
8. Provide a means to monitor and change the state of the system by
   operations staff.
   *Status: Not Satisfied*
9. Provide a choice of transfer protocols to be used, allowing the operator
   to choose a protocol
   *Status: Somewhat Satisfied*
   + [-sp-] Why? What is the operational requirement hidden in this?
     Speed? Bandwidth? Quantify.

** TADA migration from 2010 iDCI project requriements 
1. Retain existing physical resources
2. ACCEPTED. Improve upon iDCI. Qualify. Quantify
3. REJECTED. Except: will submit ingest to archive
4. ACCEPTED. Improve upon iDCI. Qualify. Quantify
5. REJECTED. If there is a bandwidth requirement, add as such.
6. ACCEPTED with caveats.
7. REJECTED. Not a requirement, but a goal I expect to happen.
8. REJECTED. Not clear.
9. REJECTED. No need.




** simulator requirements (DAFLSIM)                                :noexport:
*** First
- process for 
  + [X] DataQ
  + [X] Action
  + [X] Instrument
  + [X] monitorQ
  + [ ] externals
- Collect "final answers" for comparision to non-sim
- Support random failures (for Action)

*** Later
- specify as graph
- literate programming; spec (graph) generates code and doc
- probes at any junction (How do I specify?)
- hilite "active edge" (when data is flowing through it)



** Meta data required for ingest into archive
- [ ] PROPID
- [ ] DATE-OBS
- [ ] DTTITLE
- [ ] DTACQNAM
- [ ] DTNSANAM
- [ ] DTINSTRU
- [ ] DTTELESC
- [ ] DTSITE
- [ ] DTUTC
- [ ] DTPI
- [ ] DTSITE

from https://support.sdm.noao.edu/browse/OPS-1991

** MVP - Minimally Viable Product
These are the absolute minium requirements for a DCI replacement.
When ever possible, avoid putting anything here that is an absolutely
essential requirement. (push "would be nice" stuff into subsequent
release)

1. Baring fatal hardware failure, every file produced by instrument
   gets into archive
2. 

** Release 2
1. Each site is "independent"
   + What is a "site"?
   + How independent do they have to be? (archive depends on telescope,
     for instance)
2. Must be able to re-route around broken machines
3. Allow institutions direct access to iRODS data ("back-door")

** Deferred requirements
- *Dashboard* for monitoring health of TADA system
  + web based
- Support for analytics
  + shared results (algorithms run against data from archive)
  + loose coupling of archive data to results
  + auto expire of results (warning 1, warning 2, delete)
* TODO Open Issues
** Which files from input list ("printed" files) should get moved to archive?
  - [ ] All of them?
  - [ ] *.fits.fz?
  - [ ] *.fits?
  - [ ] *.hdr
  - DEFAULT ANSWER: only *.fits.fz and *.fits
** What if FITS files do NOT contain minimum required metadata (fields/values)?
  - Insert dummy (not realistic) values.
  - Calculate values. How?
  - Reject file (report and do not archive)
  - DEFAULT ANSWER: Reject file, move to remediation store, log error
** What are the expected workflows?
For instance:
- Load Proposal ID, etc.
- Reingest remediated files.  a) mountain, b) valley
* TODO Closed Issues
*/<NONE>/*

* Assumptions
- Number of users of an instances of this system is very small (under
  20).  "Users" in this case are data-managent operators of some
  sort.  People that make sure the data is still flowing and correct
  problems as they come up (which should be very rare).

* DEFERRED
These features are *not* implemented. They may or may not be
implemented in the future.  They are listed for 2 reasons:
1. To explicitly identify features not in the release
2. To offer candidates for future implementation

** (mountain) copy and morph
Copy the files from the ASTRO created file structure into a structure
that mirrors the old iDCI directory tree.
: /mtncache/fits/<DATE>/<TELESCOPE>/<PROPID>/<datafile>
This will require reading FITS header to get the fields and some may
not even be there.  Implications: more software packages to load, more
edge conditions.  This should be done as a complete seperate process.
I won't break anything else since its just grabbing a copy and
stashing it. 
** FPACK before transmit from Mountain to Valley
Compress FITS files before transmitting.   Since we use irsync (as of
this writing) to move files from M->V, this needs to be done in place
for all non-compressed FITS files in the directory tree *before* the
irsync is done.
** sdpost writes to /tmp/mountaincache
Might be better to write to non-/tmp directory.  But there are
security issues related to such which I didn't spend the time to
understand. Just setting the setuid bit of the backend end 
: sudo chmod u+s /usr/lib/cups/backend/sdpost 
is *not* good as CUPS traps such as a potential security hole.

Other cleanup needed in sdpost.  See reference files at top of script.

** Multi data-queues on one machine (same redis server)
Instead of two instances.  Should be one with different namespaces for
each queue.
** pass instrument data into data-flow
Currently, all information needed in the data is assumed to be
included in the single file that is posted to the start of the flow.
If instrument data is needed, it should be included in the metadata of
the file.  But if the file doesn't have sufficient support for
metadata, another mechanism is needed.  Perhaps the mechanism is
simply to zip the data and a seperate metadata file together and send
as one.  One challenge would be that different file types would
require different methods for metadata access.
* Instrument table
| Site         | Telescope | Instrument | Type                   | Prefix |
|--------------+-----------+------------+------------------------+--------|
| Cerro Pachon | SOAR      | Goodman    | spectograph            | psg    |
| Cerro Pachon | SOAR      | OSIRIS     | IR imager/spectrograph | pso    |
| Cerro Pachon | SOAR      | SOI        | image                  | psi    |
| Cerro Pachon | SOAR      | Spartan    | IR imager              | pss    |
| Cerro Pachon | SOAR      | SAM        | imager                 | psa    |
| Cerro Tololo | Blanco 4m | DECam      | imager                 | c4d    |
| Cerro Tololo | Blanco 4m | COSMOS     | spectrograph           | c4c    |
| Cerro Tololo | Blanco 4m | ISPI       | IR imager              | c4i    |
| Cerro Tololo | Blanco 4m | Arcon      | imagers/spectrographs  | c4a    |
| Cerro Tololo | Blanco 4m | Mosaic     | imager                 | c4m    |
| Cerro Tololo | Blanco 4m | NEWFIRM    | IR imager              | c4n    |
| Cerro Tololo | 1.5m      | Chiron     | spectrograph           | c15e   |
| Cerro Tololo | 1.5m      | Arcon      | spectrograph           | c15s   |
| Cerro Tololo | 1.3m      | ANDICAM    | O/IR imager            | c13a   |
| Cerro Tololo | 1.0m      | Y4KCam     | imager                 | c1i    |
| Cerro Tololo | 0.9m      | Arcon      | imager                 | c09i   |
| Cerro Tololo | lab       | COSMOS     | spectrograph           | clc    |
| Kitt Peak    | Mayall 4m | Mosaic     | imager                 | k4m    |
| Kitt Peak    | Mayall 4m | NEWFIRM    | IR imager              | k4n    |
| Kitt Peak    | Mayall 4m | KOSMOS     | spectograph            | k4k    |
| Kitt Peak    | Mayall 4m | ICE        | Opt. imagers/spectro.  | k4i    |
| Kitt Peak    | Mayall 4m | Wildfire   | IR imager/spectro.     | k4w    |
| Kitt Peak    | Mayall 4m | Flamingos  | IR imager/spectro.     | k4f    |
| Kitt Peak    | Mayall 4m | WHIRC      | IR imager              | kww    |
| Kitt Peak    | Mayall 4m | Bench      | spectrograph           | kwb    |
| Kitt Peak    | Mayall 4m | MiniMo/ICE | imager                 | kwi    |
| Kitt Peak    | Mayall 4m | (p)ODI     | imager                 | kwo    |
| Kitt Peak    | Mayall 4m | MOP/ICE    | imager/spectrograph    | k21i   |
| Kitt Peak    | Mayall 4m | Wildfire   | IR imager/spectrograph | k21w   |
| Kitt Peak    | Mayall 4m | Falmingos  | IR imager/spectrograph | k21f   |
| Kitt Peak    | Mayall 4m | GTCam      | imager                 | k21g   |
| Kitt Peak    | Mayall 4m | MOP/ICE    | spectrograph           | kcfs   |
| Kitt Peak    | Mayall 4m | HDI        | imager                 | k09h   |
| Kitt Peak    | Mayall 4m | Mosaic     | imager                 | k09m   |
| Kitt Peak    | Mayall 4m | ICE        | imager                 | k09i   |
  

* COMMENT Release checklist
Before each release, make sure the following are done.
** Maintainability 
- [ ] Documentation as built
- [ ] Requirements addressed in software as built
- [ ] Tests
- [ ] Configuration Management
- [ ] Auto provisioning of everything I develop
- [ ] Documentation of existing system
* Footnotes                                                          :draft1:

[fn:1] SDM is responsibly managing data, nothing is being lost, its
going where it should, rates and sizes of data are as expected, manual
intervention is not *required* except in the most unusual circumstances
(expected 2-4 times per YEAR). Code changes can be made with courage,
without doubt or fear of breaking something.

[fn:2] https://simpy.readthedocs.org/en/latest/

[fn:3] [[http://irods.org][iRODS]] 4.x;  4.0 was release April 4, 2014; 4.0.3 released Aug
20, 2014

[fn:4] In the NOAO case, these hosts map to the following: T1=Mayall
4m, M1=Kitt Peak, V1=Tucson, T2=SOAR, T3=Blanco 4m, M2=Cerro Pachon,
M3=Cerro Tololo; V2=La Serena

* COMMENT POSTSCRIPT
/(this section here to keep Document Comments out of the way)/
source: ~/orgfiles/designs.org

Something like this can be inserted into doc by invoking export dispatcher
and selected "insert template" (C-c C-e #).


#+TITLE:   TADA (Telescope Automatic Data Archiver) SDD
#+KEYWORDS: 
#+LANGUAGE:  en
#+OPTIONS:   H:3 num:nil toc:t \n:nil @:t ::t |:t ^:nil -:t f:t *:t <:t
#+OPTIONS:   TeX:t LaTeX:t skip:nil d:nil todo:t pri:nil tags:nil
#+INFOJS_OPT: view:nil toc:t ltoc:t mouse:underline buttons:0 path:http://orgmode.org/org-info.js
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+LINK_UP:   
#+LINK_HOME: 
#+XSLT: 


#+TAGS: draft1(1)  review(r)

* Footnotes

[fn:5] Actual directory to use is set in "/etc/tada/dq.conf"

[fn:6] A queueue "type" is one of: MOUNTAIN, VALLEY.  Some of the
configuration fields are type specific.  For instance, the
"archive_dir" field only makes sense for VALLEY machines.
 
 
