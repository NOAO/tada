* About this document                                                :draft1:
This document is intended for system software developers and
maintainers.  It may also be shared with decision makers to provide
a common context that includes requirements, goals, and plans.  The
primary audiance is employees of NOAO, but an attempt was made to keep
it general enough for use in other domains.

* Document Overview                                                  :draft1:
The software described here is: /TADA (Telescope Automatic Data Archiver)/
The purpose of TADA is to provide an end-to-end flow of data from
telescope instruments to archives at machines around the world. By its
very nature, TADA is a multi-machine system.  The *general purpose*
might be described as:
   #+BEGIN_QUOTE
   Collect data from remote instruments and save in nearby
   cache. Transfer cache data across network with dubiuos reliability
   to a local archive. Remove cache data after it has been
   successfully transferred.
   #+END_QUOTE

This System Design Document (SDD) attempts to describe software as
built. Since the document and software are being written at the same
time, they may not match until the project is completed. (Maybe not
even then!  If in doubt ask me (Steve P).

The project must proceed without the benefit of known
requirements. Therefore, development will proceed with a series of
alpha releases that represent various "threads through the system".
Each thread may contain arbitrary amounts of mock-component code but
will at the very least simulate the end-to-end flow of data.  Later
alpha-versions will contain progressively more real (non-mockup)
components. The first beta release will contain no mockup-components
for the thread but may contain mockup-components on either side of the
thread. e.g. A mockup-component will pretend to be telescope
instruments sending data.

At each alpha-release, stake-holders have the opportunity to affect
what is done for the next release.  Its *very important* for
stake-holders to provide prompt feedback when needed.  In the absence
of such feedback, developers will simply guess on what is important
next and move on.

We refer to "mountain" and "valley" sites/machines.  These aren't
necessarily to be taken literally. "Mountain" means the computer(s)
connected to telescope instruments directly or through LAN. The
connection is assumed to be unlikely to be lost unless the
instruments and computers go down also.  "Valley" means the
computer(s) far away from the telescope instruments and connected
only via the internet. 

The software described here is intended to solve a general problem --
collecting remote data and storing in an archive without loss in the
face of failures in hardware and software.  But the document focuses
on that problem in a more specific domain (optical astronomy data).
Where there are domain specific issues, they are described elsewhere.
Links in this document may refer to those domain specific issues. For
non-NOAO readers, those links will be unresolvable.  If you are
reading this for an application that does not involve mountain-top
telescopes, you should be able to translate.  For a more general
application or domain, you can roughly think:
  - telescope :: "data collection instrument"
  - mountain-top :: "collection point for several instruments"
  - Mountain Cache :: "storage for several instruments" that is very
                      reliably connected to the instruments
  - Mountain Mirror :: "off-site storage for several instruments". Not
       reliably connected to instruments. 

* TODO Design Overview
The TADA system consists of a set of processes that communciate with
each other across multiple machines. 

** TERMS
Terms relevent to this section:
- Mountain Cache :: temporary storage for all data files accepted from
                    any telescope on the same mountain as the cache.
- Mountain Mirror :: duplicate of the Mountain Cache, but moved off of
     the mountain (to someplace logicaly near the archive). Cache and
     Mirror are seperated by a possibly unreliable network connection.
- Archive Staging :: data files vetted, scrubbed, and ready for archive ingest
- Mitigate :: data files that need to be corrected before they can go to
              Archive Staging
- Non-Archive :: files not suitable for archival. They will be deleted
   from here using a First In First Out (trashed) method.
- Transfer Queue :: Data-queue whos contents represent data files that
                    need to be moved from Mountain Cache (on
                    Mountain) to Mountain Mirror (on Valley).
- Submit Queue :: Data-queue whos contents represent data files that
                  that have been submitting for saving. File types
                  that are not appropriate for the archive, will be
                  moved to a non-archive store. Else they'll be
                  put in archive (if possible) or in "Mitigation
                  Queue" (if invalid for archive submit)
- Mitigation Queue :: Data-queue whos contents represent data files
     that should find there way to the archive, but have something
     wrong with them.  After they have been manually modified, they
     should be put back on the Submit Queue

** ASTROPOST Process: Store file submitted via "lp" in Mountain Cache
*** Summary
Captures files sent from telescope via "print".
*** Description
From the telescope, a user or program uses the command:
: lp -d astro <filename>
to submit data.  This process is a [[http://www.cups.org/documentation.php/man-backend.html][CUPS backend]] ("astropost") and honors
the CUPS API.  It simply copies the file to a location under the mountain
cache root directory that is determined by backend parameters (user,
jobid, etc.) and adds the full name of the moved file (with checksum)
to the Transfer Queue.

*** Preconditions
The "lpd" protocol handling of CUPS must be enabled (it isn't enabled
by default).

*** Postconditions
The mountain cache directory tree is populated with printed files.
Uniqueness *of path* is maintained by the combination of jobid and
username. Its still possible (likely) for there to be duplicates of
base filenames in the tree.

** Process: Transfer content of Mountain Cache to Mountain Mirror
*** Summary
Transfer all data from Mountain (Mountain Cache) to Valley (Mountain Mirror).

*** Description
When simple, avoid transfering large files that have already been
successfully transfered. (e.g. rsync)

** Process: Morph Mountain Mirror to Archive Stage
*** Summary
Renames and copies files from the Mountain Mirror directory tree to
the Archive Stage directory tree.

*** Description
Uses fields from FITS headers to create new fields and create a filename
that satisfies the [[http://ast.noao.edu/data/docs][file naming convention]].  Maintains "backward
pointing" fields in the FITS header so that the path to the same data
on the Mountain Cache can be reproduced.

*** Preconditions
Mountain Mirror directory tree is on local machine file system.

*** Postconditions
All data files from the Mountain Mirror exist in one of three
places. See TERMS above for description of what these contain.
1. Archive Staging
2. Mitigate 
3. Non-Archive 

** Process: Remove confirmed transfers from Mountain Cache
*** Summary
Remove files from the Mountain Cache if they can be confirmed to exist
on the Valley machine (in Mountian Mirror). 

*** Description
Use checksum comparison to determine if file was transfered ok.
There may be considerable delay between when a file appears in the
Mountain Mirror and it is deleted from the Mountain Cache.  (But don't
count on it!) Mountian machines must have sufficient storage to
weather a long disconnect between Mountain and Valley machines. 

* TODO Config
** iinit
irodsHost valley
irodsPort 1247
irodsUserName rods
irodsZone tempZone
** irod directory structure

#+BEGIN_EXAMPLE
/sd_zone/
        from_cache/
        for_archive/
#+END_EXAMPLE

* As-Built
** General
This section documents specific builds.  When a requirement or feature
is described outside of the /As-Built/ section, it should be
considered a future possibility, *not* something that has been
implemented. 

I record dated sub-sections below but will typically hide all but the
most recent.  Ask if you want older sections for some reason.

** Touch Points
- INPUT queue
  via lpd protocol
- INPUT to submit queue (after mitigation, for pipeline)
- OUTPUT to archive ingest
** Changes from iDCI
- All files "printed" to printer "astro" are sent to valley (not just
  selected types)
- No gratuitous waiting or "spinning"!
  Data flow from "print" to submit to archive never involves arbitrary
  wait. The flow is data driven, so that as soon as one process
  finishes with it, the next process does its job (provided it isn't
  already working on another file).  No CRON jobs are used for any of
  the main data flow.  Some CRON may be introduced for optional pieces
  (such as monitoring). 
  
** <2014-11-11 Tue>
- Attempt to post a duplicate file will be ignored
  + "duplicate" is determined by checksum of content. Filename is irrelevent.
- The same filename with different content can be printed to
  "astro". Since the full pathname makes use of user and job id, no
  collision will occur in Mountain Cache or Mountain Mirror.
- Upon successful transfer of a file from Cache to Mirror, the file
  will be immediately removed from the Cache. (if longer lived copies
  are wanted on the mountain, they can be done with a seperate process).
- On failure to transfer a file from Cache to Mirror, the file will be
  retained in the Cache and retained on the transfer queue with an
  incremented error count.
** COMMENT <2014-10-24 Fri>
*** Thread-2: Touches FITS data  (verifies selected metadata in archive)
Given a "source directory" tree that may contain FITS files, 
*** Open Issues
- Which files from input list ("printed" files) should get moved to archive?
  + DEFAULT ANSWER: only *.fitz.fz

- What if a FITS file does NOT contain minimum required metadata?
  + DEFAULT ANSWER: Reject file, move to remediation store, log error

- What is the minium required metadata?
  + DEFAULT ANSWER: Presence of following fields in FITS hdr without
    regard to their value:
    - DATE-OBS
    - DTACQNAM
    - DTINSTRU
    - DTNSANAM
    - DTPI
    - DTSITE
    - DTSITE
    - DTTELESC
    - DTTITLE
    - DTUTC
    - PROPID

** Caveats and Warnings
- Assume irods documentation is correct when it says that transfers
  are guaranteed using checksum.  I have not done an experiment to
  prove this.
- It is possible for a queue push to fail (perhaps the queue service
  was killed). If so, there may be items in the associate storage that
  are not in the queue.  See "Deferred" below for how to handle this case.
** Deferred
- Process to monitor error counts on queues.  Demand human attention
  for any files that get high (config setting) error count.
- Process to compare queue and associated data storage.  Add items to
  queue that aren't there already but are in storage.

* Sprint user stories
These are the expect outcomes from progressively more complex [[https://www.scrum.org/][scrum]] sprints.

In our case "user" means two kinds of people: 
  1. scientist that want access to data,
  2. SDM DevOps employees that need to manage the process

** Thread-1: Establishes file move to archive and test
This is minimal "thread through the system" starting at raw-data and
terminating with files in the archive.
- [X] mock-LPR;  Feed each file in list to Ingest after random delay
- [X] Ingest;  Copy file into mock-IRODS (a local filesystem)
- [X] Test;  Verify all input files are  in mock-IRODS

*** 
#+BEGIN_SRC dot :file figures/thread1.png :cmdline -Tpng :tangle src-tangles/thread1.dot
  digraph thread1 {
      rankdir="LR";
      edge [len=1.0];
      raw [shape="invhouse"];
      expected [label="Expected\n(raw)", shape="invhouse"];
      report [shape="house"];

      raw -> mockLpr -> ingest -> archive -> test;
      timing -> mockLpr;
      expected -> test -> report;
  }
#+END_SRC

** Thread-2: Touches FITS data  (verifies selected metadata in archive)
- [X] all of Thread-1
- [X] only transfer files matchin: *.fits.fz 
- [X] insure minimum (level 0) set of required metadata fields in FITS
  + minimum acceptable for archive
- On inadequate metadata:
  - [X] reject (don't archive) 
  - [ ] move to remediation store
  - [ ] log error
- [X] Test;  Verify all files in mock-IRODS contain required metadata;

*** 
#+BEGIN_SRC dot :file figures/thread2.png :cmdline -Tpng :tangle src-tangles/thread2.dot
  digraph thread2 {
      rankdir="LR";
      edge [len=1.0];
      raw [shape="invhouse"];
      expected [label="Expected\n(cooked)", shape="invhouse", fontcolor="green"];
      report [shape="house"];

      raw -> mockLpr -> ingest;
      ingest -> archive [label="insert metadata", fontcolor="green"];
      archive -> test;
      timing -> mockLpr;
      expected -> test -> report;
   }
#+END_SRC
    
** Thread-3: Split into 2 machines, use iRODS client/server
- [ ] mock-LPR;  Feed each file in list to Ingest after random delay
- [ ] Ingest; add file to iRODS[fn:3] on remote machine
- [ ] Verify integrity of file across machines (checksum)
  + Retry N times if integrity violated
- [ ] Test; Verify all iRODS filesystem contains everything from orig
  filesystem



*** 
#+BEGIN_SRC dot :file figures/thread3.png :cmdline -Tpng :tangle src-tangles/thread3.dot
  digraph thread3 {
      rankdir="LR";
      edge [len=1.0];
      raw [shape="invhouse"];
      expected [label="Expected\n(cooked)", shape="invhouse", fontcolor="green"];
      report [shape="house"];
      archive [label="Archive\n(iRODS)", shape="box"];

      subgraph cluster_mountain {
        label = "Mountain";
        style="dashed";

        timing -> mockLpr;
        raw -> mockLpr -> ingest;
      }

      subgraph cluster_valley {
        label = "Valley";
        style="dashed";

        ingest -> archive [label="iCommands", fontcolor="green"];
        archive -> test;
        expected -> test -> report;
      }
   }
#+END_SRC

** LATER
- easy to add plugins for scientists 
  + scientist provides program to run against (filtered) set of
    images, stores "result" file accessable in archive
* Classes of supporting machines (hosts)
The machines that are used in the TADA system can be categorized into
the following classes of hosts. The software that runs on each machine
of the same class should be identical and come from a single source
code repository.  Any difference between the behavior of
software on different machines of the same class comes from
configuration files unqiue to the machine.

 - T :: Telescope; The machine from which observer does the "print". We
        *can't touch this* except to add a printcap entry.
 - M :: Mountain cache; Contains all instrument data that hasn't
        successfull made it into the archive. And maybe some that has.
 - V :: Valley; The first stop of data coming from Mountain
 - A :: Archive; the final resting place of the data made available to
        scientists. We *can't touch this* directly. Only by "submit to
        ingest". 

Roughly, data flows top to bottom through the classes of machines
listed above.  Meaning; data is generated at the Telescope, gets
collected at Mountain cache, then transfered to the Valley, and
finally scrubbed and submitted to the Archive.

There are more than one instance of each of these classes of hosts, so
things get a little more complicated with regard to collecting and
distributing. 

Here's a rough schematics of what we end up with.  Arcs represent data
flow.  Note that data only flows bewteen "adjacent" classes of hosts.[fn:4]

#+BEGIN_SRC dot :file figures/general-machine-schematic.png :cmdline -Tpng :tangle src-tangles/thread1.dot
  digraph schematic1 {
      rankdir="LR";

      T1 -> M1 -> V1 -> A1 ;
      T2 -> M2;
      T3 -> M3;
      {M2;M3} -> V2 -> A2 ;
      A1 -> A2 -> A1;
  }
#+END_SRC

* Goals                                                              :draft1:
** Prove its done right
To PROVE we have it right[fn:1], we need good monitoring. To support
courageous code changes, the monitoring should be nearly identical
between:
- production
- developmental (to be deployed) system (on VMs or real machines)
- under DES (Discrete Event Simulation)[fn:2]
  [[~/sandbox/dfsim/dfsim.py][dfsim]]
** Easy to maintain
Create a system that can be maintained using no more than 25% of one
full time employee.  We expect maintenance to include:
- correcting problems in FITS files stored in Mitigation queue and store
- replacing broken hardware (disks, computers) and installing required
  software from scratch

*** Simulation                                                     :noexport:
It would be GREAT to generally connect simulator to data-flow graph
display. What tools?  Need graphics that support drawing graph and can
hilite nodes. tcl/tk?  Is there something in latest networkx that
helps? Perhaps I need to write a general OSS project.  Lauch with
graph. It draws.  Pipe in for commands (hilite, others?). Pipe out for
state?

*** Monitor display                                                :noexport:
Plots from DES (gnu plot?) to represent values of resources (queue
size).  Alerts for when thresholds exceeded. (queue max size reached)
Utilization measures.
* Secondary Goals                                                    :draft1:
My primary goal is to develop useful software.  Exactly what that
software will be is unfolding.  It has to be an iterative process. But
regardless of what the software is, there are some secondary goals
that go along with it. Here are most of them:

1. Documentation as built

   My intent is to provide "as built" design and code documentation. Code
   documentation will be generated directly from annotated code. Design
   docs will be hand written, with diagrams.  It will include example
   runs with inputs and outputs listed. The intended reader for both is
   someone that is software tech savvy.

2. Requirements addressed in software as built

   Whatever I develop is intended to address some requirements that I
   have in mind.  I'll put those down in a document.  These may be
   different than any requirements anyone gives to me because they will
   be directly focused on functionality of the software I develop, rather
   than on a larger system perspective (which I may have little control
   over). The intended reader is management and/or software engineer.

3. Tests

   Each package I write has a "smoke test".  This is a simple script that
   can be run by anyone after the software is installed to see that it
   works in some fashion.  My smoke tests are not exhaustive regression
   tests.  They are intended to be used by developers to ask the
   question: "did I break anything with the last change". Smoke tests
   include their own test data and are checked into configuration
   management with the code.

4. Configuration Management

   All my software will be checked into github or bitbucket. Related
   documentation will be included with the code.

5. Auto provisioning of everything I develop

   I'll provide a "vagrant box", or similar, for all my stuff.  This will
   allow a new Virtual Machine(s) to be created from scratch and all my
   stuff installed on it such that my smoke tests will work on the new
   VM(s).

6. Documentation of existing system

   In the process of figuring out what my new stuff has to do, I have to
   figure out what the existing stuff does. I don't want to attempt to
   hold all that in my head, so I document it.  You've all seen at least
   part of my DCI "notes". That is basically the source of what I'm
   talking about here.  I don't intend to formalize it any way unless
   forced into it. I think it would be too time-consuming/expensive for
   me to do and I think I have more the enough technical work on my
   plate.  But I will provide at least a crude extraction from my notes
   to something that might be useful to others.  The effort I put into
   such depends on feedback from you. No feedback means I'll provide
   something that is a similar level of informality as the notes I've
   already shared with you. I've already exported some parts of that
   (like my diagram) to the opswiki.

7. Keep It Super Simple
   
   Work very hard to keep the structure of the system and code
   simple.  If there is a temptation to "optimize", make sure its
   worth it. To be worth it, there must be an existing case of
   inadequate performance and a requirement must exist to perform at a
   quantifiable level that the better than the current one. Before
   changing code, *measure* the system to identify where the ill
   performing area is.  Don't add optimatizations unless they are
   *proven* to help meet requirements.

---------

* TODO Requirements
** General systemic requirements
1. Provide all required the functionality of system this replaces
2. Resilient 
   - don't break -- EVER
3. Maintainable
   - by new employees without large learning curve
4. Operate fast enough (need quantification)

** Candidate requirements
These requirements have *not been committed to*.  In many case they
need to be made more precise.

- All database clients must be capable of reconnecting to database
  servers on connection loss (so components can be restarted)
- Increase level of automation of regular operation functions
- use version control always; with commit comments
- elliminate direct changes to live production system (from tagged version)
- (document minimumaly acceptable coding style)
- Implement regression testing (automated where possible, documented otherwise)
- write design documentation
- write installation documentation
- write usage documentation
- reproducible installs
- daily operations must not require manual intervention
- daily operations must not require human monitoring (automatic alerts instead)
- eliminate metadata remmediation in its present form (what form???)
  + get metadata from file format, or
  + get metadata from TO/observer/observatory support staff at data
    collection time
- insert "archival metadata" just before final archiving (???)
- insert of archival metadata should be idempotent
- eliminate mountain copy coherency requirement (???)
- filename agnostic; nothing in the system should depend on the
  structure or uniqueness of a filename
- limit access to internals connection points (ports, databases)
  + perhaps by host, port, user
- literate programming: data flow software and config files: must be
  able to auto generate a document that describes the flow (including
  connectivity or data-flow diagram).
- Continue to store on mountain if connection to valley is severed.
  + How long? [DEFAULT ANSWER: 7 days]
  + Automatically dump stored mountain data to valley when connection
    restored
- Mountain machines run unattended. Disk "never" overflows.
  + Data that has been successfully transfered to valley is deleted
    from mountain.
  + If connection to valley remains severed for extend time and data
    continues to be collected on mountain, data will be lost.  How?
    [DEFAULT ANSWER: oldest will be thrown away first]
- Data submitted to NSA (archive) must have PROPID that is in the NSA
  metadata-DB
  + How is NSA metadata-DB retrieved
  + What if PROPID is not in metadata-DB? [DEFAULT ANSWER: File is
    moved to remediation store; error logged; no ingest happens]
- Handle "typical" failure modes gracefully with no loss of data:
  + reboot of any machine at any time [IMPORTANT - automate test?]
  + Lost of DNS
  + filesystem corruption (within "reason")
- Verify no errors on submit of file to archive (NSA) via socket
  + How?
  + What does NSA return back?  Does it return error for every case in
    which file is not archived?
- Same version of iRODS in TADA as NSA?
  + Not required if API is identical for used commands. 
  + [[http://irods.org/][iRODS]] says that version 3.+ and 4.+ can be combined in one collection
- Security ???
  + firewalls configured to only allow access to key ports from
    trusted hosts
- Files must be renamed according to TBD scheme before submit to
  archive
  + How is name derived? 
  + Assume name is derived from header -- but this limits to
    processing of FITS (known header info) only.
- allow disabling of auto cache-file expiration
- on "submit to archive" retry N times (N given by config file)
- tests to include simulation of irods stop-delay-start

** From 2010 iDCI project definition
(minor editing done on language of requirements)

#+BEGIN_EXAMPLE
iDCI: Integrated Data Cache Initiative
Version 0.1 (02/24/2010)
The [[http://chive.tuc.noao.edu:8080/DPPDOCS/operations-documentation/software-system/application-components/noao-e2e/e2ev1.5/iDCI_project_definition.pdf/at_download/file][PDF]] contains a bit more detail on each requirement.
#+END_EXAMPLE

*Status* below is per Irene.  Some might not be true anymore. 

1. Retain the existing DCI configuration, physical and logical
   resources. 
   *Status: Satisfied*
   + [-sp-] Need to retain physical resources, but why the logical ones?
2. Implement design changes that lower Operations maintenance while
   maintaining the overall functionality of the existing DCI.
   *Status: Not Satisfied*
3. Provide an interface for external E2E boundary objects.
   *Status: Satisfied*
   + [-sp-] I don't see a well defined/documented interface.
4. Guarantee the reliable and immutable transfer of data between all start and
   end points controlled by the iDCI.
   *Status: Satisfied*
   + [-sp-] Not happening, unless requirement allows for manual fudging
5. Maximize use of available bandwidth for bulk data transfer without
   interfering significantly with normal network traffic.
   *Status: Satisfied*
   + [-sp-] Why?  Certainly not "maximized" (maybe "improved")
6. Persist the state of pending data transfers across network outages, system
   failures and unexpected crashes of the software, recovering automatically once local or
   remote services become available.
   *Status: Satisfied*
   + [-sp-] Not happening.  People regularly have to start/restart pieces.
7. Be configurable to as to provide flexible routing of data to alternate sites.
   *Status: Satisfied*
   + [-sp-] At what touch point?  I don't see any way of doing this
     simply by changing a config file.
8. Provide a means to monitor and change the state of the system by
   operations staff.
   *Status: Not Satisfied*
9. Provide a choice of transfer protocols to be used, allowing the operator
   to choose a protocol
   *Status: Somewhat Satisfied*
   + [-sp-] Why? What is the operational requirement hidden in this?
     Speed? Bandwidth? Quantify.

** TADA migration from 2010 iDCI project requriements 
1. Retain existing physical resources
2. ACCEPTED. Improve upon iDCI. Qualify. Quantify
3. REJECTED. Except: will submit ingest to archive
4. ACCEPTED. Improve upon iDCI. Qualify. Quantify
5. REJECTED. If there is a bandwidth requirement, add as such.
6. ACCEPTED with caveats.
7. REJECTED. Not a requirement, but a goal I expect to happen.
8. REJECTED. Not clear.
9. REJECTED. No need.




** simulator requirements (DAFLSIM)                                :noexport:
*** First
- process for 
  + [X] DataQ
  + [X] Action
  + [X] Instrument
  + [X] monitorQ
  + [ ] externals
- Collect "final answers" for comparision to non-sim
- Support random failures (for Action)

*** Later
- specify as graph
- literate programming; spec (graph) generates code and doc
- probes at any junction (How do I specify?)
- hilite "active edge" (when data is flowing through it)



** Meta data required for ingest into archive
- [ ] PROPID
- [ ] DATE-OBS
- [ ] DTTITLE
- [ ] DTACQNAM
- [ ] DTNSANAM
- [ ] DTINSTRU
- [ ] DTTELESC
- [ ] DTSITE
- [ ] DTUTC
- [ ] DTPI
- [ ] DTSITE

from https://support.sdm.noao.edu/browse/OPS-1991

** MVP - Minimally Viable Product
These are the absolute minium requirements for a DCI replacement.
When ever possible, avoid putting anything here that is an absolutely
essential requirement. (push "would be nice" stuff into subsequent
release)

1. Baring fatal hardware failure, every file produced by instrument
   gets into archive
2. 

** Release 2
1. Each site is "independent"
   + What is a "site"?
   + How independent do they have to be? (archive depends on telescope,
     for instance)
2. Must be able to re-route around broken machines
3. Allow institutions direct access to iRODS data ("back-door")

** Deferred requirements
- *Dashboard* for monitoring health of TADA system
  + web based
- Support for analytics
  + shared results (algorithms run against data from archive)
  + loose coupling of archive data to results
  + auto expire of results (warning 1, warning 2, delete)
* TODO Open Issues
** Which files from input list ("printed" files) should get moved to archive?
  - [ ] All of them?
  - [ ] *.fits.fz?
  - [ ] *.fits?
  - [ ] *.hdr
  - DEFAULT ANSWER: only *.fits.fz and *.fits
** What if FITS files do NOT contain minimum required metadata (fields/values)?
  - Insert dummy (not realistic) values.
  - Calculate values. How?
  - Reject file (report and do not archive)
  - DEFAULT ANSWER: Reject file, move to remediation store, log error
** What are the expected workflows?
For instance:
- Load Proposal ID, etc.
- Reingest remediated files.  a) mountain, b) valley
* TODO Closed Issues
*/<NONE>/*

* Assumptions
- Number of users of an instances of this system is very small (under
  20).  "Users" in this case are data-managent operators of some
  sort.  People that make sure the data is still flowing and correct
  problems as they come up (which should be very rare).

* DEFERRED
These features are *not* implemented. They may or may not be
implemented in the future.  They are listed for 2 reasons:
1. To explicitly identify features not in the release
2. To offer candidates for future implementation

** (mountain) copy and morph
Copy the files from the ASTRO created file structure into a structure
that mirrors the old iDCI directory tree.
: /mtncache/fits/<DATE>/<TELESCOPE>/<PROPID>/<datafile>
This will require reading FITS header to get the fields and some may
not even be there.  Implications: more software packages to load, more
edge conditions.  This should be done as a complete seperate process.
I won't break anything else since its just grabbing a copy and
stashing it. 
** FPACK before transmit from Mountain to Valley
Compress FITS files before transmitting.   Since we use irsync (as of
this writing) to move files from M->V, this needs to be done in place
for all non-compressed FITS files in the directory tree *before* the
irsync is done.
** sdpost writes to /tmp/mountaincache
Might be better to write to non-/tmp directory.  But there are
security issues related to such which I didn't spend the time to
understand. Just setting the setuid bit of the backend end 
: sudo chmod u+s /usr/lib/cups/backend/sdpost 
is *not* good as CUPS traps such as a potential security hole.

Other cleanup needed in sdpost.  See reference files at top of script.

* COMMENT notes for Sean
I've had trouble diagnosing when something goes wrong with
"printing".  Trouble probably from:
- CUPS is security minded so permissions are tricky
- what gets logged and where it gets logged is often unclear to me
- some of the initial futzing to get things going (that probably won't
  occur in maintenance mode)
- print queue is automatically disabled (this can be changed through
  cups config) when any print error occurs.  Since I was debugging,
  this happened alot.  Each time, had to clear print jobs, re-enable.

* COMMENT New Name
** Possible names for DCI replacement
- [ ] MADI :: Mountain Archive Data Initiative
- [ ] MATT :: Mountain Archive Telescope Transport
- [ ] ADAM :: Archive Data Automated Mover
- [X] TADA :: Telescope Automatic Data Archiver
- [ ] MAMA :: Mountain Automated Moving Archive
- [ ] TATO :: Telecsope Archive Transport Operation
- [ ] DRAT :: Data Relay Archive Transporter
- [ ] MAMI :: Mountain Archive Mover Initiative
- [ ] MOTA :: MOuntain To Archive
- [ ] STARI :: Send Telescope Archive Relay Initiative
- [ ] STARE :: Send Telescope data via RElay

** COMMENT Keyword terms for acro
archive
automated
data
initiative
irods
mountain
mover
operation
relay
send
telescope
transport
** COMMENT For mountain (iSTB etc) replacement
ASTRO: Automated Submission To Repository,Observations
CACHED: Capture And Cache HEavenly Data
 
The original was called STB: Save The Bits  (very bad name)

The purpose of this piece:
Allow an observer at an observatory to "print" their data (image, picture, spectograph) to the mountain cache and to the archive  This is essentially a way of saving their data to a database.

I'm much more fuzzy on what the boundary of this piece is so my words are questionable:
Save the data collection
instrument to database/archive
print to archive
instrument to cache
Cache it
Submit this to repository/archive
Spool my images/data/collection
Capture In Archive (CIA?)/Database/Cache

Capture And Cache HEavenly  Data (CACHED)

* COMMENT Release checklist
Before each release, make sure the following are done.
** Maintainability 
- [ ] Documentation as built
- [ ] Requirements addressed in software as built
- [ ] Tests
- [ ] Configuration Management
- [ ] Auto provisioning of everything I develop
- [ ] Documentation of existing system
* Footnotes                                                          :draft1:

[fn:1] SDM is responsibly managing data, nothing is being lost, its
going where it should, rates and sizes of data are as expected, manual
intervention is not *required* except in the most unusual circumstances
(expected 2-4 times per YEAR). Code changes can be made with courage,
without doubt or fear of breaking something.

[fn:2] https://simpy.readthedocs.org/en/latest/

[fn:3] [[http://irods.org][iRODS]] 4.x;  4.0 was release April 4, 2014; 4.0.3 released Aug
20, 2014

[fn:4] In the NOAO case, these hosts map to the following: T1=Mayall
4m, M1=Kitt Peak, V1=Tucson, T2=SOAR, T3=Blanco 4m, M2=Cerro Pachon,
M3=Cerro Tololo; V2=La Serena

* COMMENT POSTSCRIPT
/(this section here to keep Document Comments out of the way)/
source: /home/pothiers/orgfiles/designs.org

Something like this can be inserted into doc by invoking export dispatcher
and selected "insert template" (C-c C-e #).


#+TITLE:   TADA (Telescope Automatic Data Archiver) SDD
#+AUTHOR:    Steve Pothier
#+EMAIL:     pothier@noao.edu
#+KEYWORDS: 
#+LANGUAGE:  en
#+OPTIONS:   H:3 num:nil toc:t \n:nil @:t ::t |:t ^:nil -:t f:t *:t <:t
#+OPTIONS:   TeX:t LaTeX:t skip:nil d:nil todo:t pri:nil tags:nil
#+INFOJS_OPT: view:nil toc:t ltoc:t mouse:underline buttons:0 path:http://orgmode.org/org-info.js
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+LINK_UP:   
#+LINK_HOME: 
#+XSLT: 


#+TAGS: draft1(1)  review(r) 
